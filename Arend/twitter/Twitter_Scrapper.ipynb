{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import folium\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_twitter_auth():\n",
    "    consumer_key = 'MqOzyOLVGp5VwMkYvcg839kVS'\n",
    "    consumer_secret = 'rLCrItfm2csmNm7o9WbCRnkVzJ9MXlKVPxbVQS0cCwgA4cfbgy'\n",
    "    access_token = '87460496-bq29UUPlMH5GSPEiT7J5TIMkosuwdSWLrTxi22zfT'\n",
    "    access_secret = 'PxmDK6vrTi30JHjeGHU9yOXviFW80WKroJUvGRGqH97hK'   \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth\n",
    "\n",
    "def get_twitter_client():\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getHashtags(tweet):\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('hashtags', [])\n",
    "    return [tag['text'].lower() for tag in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashtagAmount(data):\n",
    "    tags = []\n",
    "    tagCount = defaultdict(int)\n",
    "    for hashtag in data['hashtags']:\n",
    "        try:\n",
    "            tags.append(hashtag.replace('[', '').replace(']', '').split(','))\n",
    "        except:\n",
    "            tags.append([])\n",
    "    \n",
    "    for tag in tags:\n",
    "        amount = len(tag)\n",
    "        tagCount[amount] += 1\n",
    "    \n",
    "    #Calculations\n",
    "    with_hashtags = sum([count for n_of_tags, count in tagCount.items() if n_of_tags > 0])\n",
    "    no_hashtags = tagCount[0]\n",
    "    total = no_hashtags + with_hashtags\n",
    "    with_hashtags_percent = \"%.2f\" % (with_hashtags / total * 100)\n",
    "    no_hashtags_percent = \"%.2f\" % (no_hashtags / total * 100)\n",
    "    \n",
    "    #Print results\n",
    "    print(\"{} tweets without hashtags ({}%)\".format(no_hashtags, no_hashtags_percent))\n",
    "    print(\"{} tweets with at least one hashtag ({}%)\".format(with_hashtags, with_hashtags_percent))\n",
    "    \n",
    "    for tagCount, tweet_count in tagCount.items():\n",
    "            if tagCount > 0:\n",
    "                percent_total = \"%.2f\" % (tweet_count / total * 100)\n",
    "                percent_elite = \"%.2f\" % (tweet_count / with_hashtags * 100)\n",
    "                print(\"{} tweets with {} hashtags ({}% total, {}% elite)\".format(tweet_count,\n",
    "                                                                                 tagCount,\n",
    "                                                                                 percent_total,\n",
    "                                                                                 percent_elite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commonHashtags(fname):\n",
    "    \"\"\"\n",
    "    Get most common hashtags for input jsonl file.\n",
    "    \"\"\"\n",
    "    with open(fname, 'r') as f:\n",
    "        hashtags = Counter()\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            hashtags_in_tweet = getHashtags(tweet)\n",
    "            hashtags.update(hashtags_in_tweet)\n",
    "\n",
    "        for tag, count in hashtags.most_common(5):\n",
    "            print('{}: {}'.format(tag, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapeTwitter(companies, client):\n",
    "    for comp in companies:\n",
    "        fname = '{}_twitter.jsonl'.format(comp)\n",
    "        with open(fname, 'w') as f:\n",
    "            for page in Cursor(client.user_timeline, screen_name=comp, count=200).pages(16):\n",
    "                for status in page:\n",
    "                    f.write(json.dumps(status._json) + '\\n')\n",
    "    \n",
    "        makeCSV(fname, comp)\n",
    "        #os.remove(fname)\n",
    "        print('{} scraped successfully!\\n'.format(comp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeCSV(fname, comp):\n",
    "    with open(fname, 'r') as f:\n",
    "        with open('{}_twitter.csv'.format(comp), 'w') as file:\n",
    "            w = csv.writer(file)\n",
    "            w.writerow(['company', 'created_at', 'favorite_count', 'retweet_count',\n",
    "                        'reply_to', 'content', 'hashtags', 'retweet', 'quote'])\n",
    "\n",
    "            for line in f:\n",
    "                tweet = json.loads(line)\n",
    "                company = comp\n",
    "                \n",
    "                created_at = datetime.datetime.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "                created_at = created_at.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "                \n",
    "                favorite_count = tweet['favorite_count']\n",
    "                retweet_count = tweet['retweet_count']\n",
    "                reply_to = tweet.get('in_reply_to_screen_name', None)\n",
    "                content = tweet['text']\n",
    "                hashtags = None if getHashtags(tweet) == [] else getHashtags(tweet)\n",
    "                retweet = tweet.get('retweeted', False)\n",
    "                quote = tweet.get('is_quote_status', False)\n",
    "                \n",
    "                w.writerow([company, created_at, favorite_count, retweet_count,\n",
    "                            reply_to, content, hashtags, retweet, quote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeGeoJson(jsonl_file, name):\n",
    "    with open(jsonl_file, 'r') as f:\n",
    "        geo_data = { \n",
    "          \"type\": \"FeatureCollection\", \n",
    "          \"features\": [] \n",
    "        } \n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            try:\n",
    "                if tweet['coordinates']:\n",
    "                    geo_json_feature = { \n",
    "                        \"type\": \"Feature\", \n",
    "                        \"geometry\": { \n",
    "                            \"type\": \"Point\", \n",
    "                            \"coordinates\": tweet['coordinates']['coordinates'] \n",
    "                            }, \n",
    "                        \"properties\": { \n",
    "                            \"text\": tweet['text'], \n",
    "                            \"created_at\": tweet['created_at'] \n",
    "                            } \n",
    "                        } \n",
    "                    geo_data['features'].append(geo_json_feature) \n",
    "            except KeyError: \n",
    "                # Skip if json doc is not a tweet (errors, etc.) \n",
    "                continue\n",
    "\n",
    "        with open('{}_twitter_geodata.jsonl'.format(name), 'w') as fout:\n",
    "            fout.write(json.dumps(geo_data, indent=4))\n",
    "\n",
    "def make_map(geojson_file, map_file): \n",
    "    tweet_map = folium.Map(location=[50, 5], zoom_start=5)\n",
    "    marker_cluster = folium.MarkerCluster().add_to(tweet_map) \n",
    "    geodata = json.load(open(geojson_file)) \n",
    "    for tweet in geodata['features']: \n",
    "        tweet['geometry']['coordinates'].reverse() \n",
    "        marker = folium.Marker(tweet['geometry']['coordinates'], \n",
    "                           popup=tweet['properties']['text']) \n",
    "        marker.add_to(marker_cluster) \n",
    "    return tweet_map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARENDAutomation scraped successfully!\n",
      "\n",
      "BLUMENBECKER scraped successfully!\n",
      "\n",
      "Beckhoff_DE scraped successfully!\n",
      "\n",
      "Pilz_INT scraped successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "companies = ['ARENDAutomation', 'BLUMENBECKER', 'Beckhoff_DE', 'Pilz_INT'] #twitter username without @ sign!\n",
    "client = get_twitter_client()\n",
    "\n",
    "scrapeTwitter(companies, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
